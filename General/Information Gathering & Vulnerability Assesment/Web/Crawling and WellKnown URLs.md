# Crawling Overview

Crawling, or spidering, is the automated process of systematically browsing the web to discover and index web pages. Web crawlers, using pre-defined algorithms, follow links from one page to another, collecting information for purposes like search engine indexing, data analysis, or web reconnaissance.

## How Web Crawlers Work

Crawlers start with a seed URL, fetch the page, and extract its links. These links are added to a queue and crawled iteratively, allowing the crawler to explore an entire website or even a large portion of the web.

### Example Crawl

1. **Homepage**: Contains links to `link1`, `link2`, and `link3`.
2. **Visiting `link1`**: Reveals more links (`link4`, `link5`), and so on.
3. **Continuing the Crawl**: The crawler systematically follows these links, gathering accessible pages and their links.

## Crawling Strategies

- **Breadth-First Crawling**: Explores all links on the seed page first, then moves to the links on those pages. Useful for getting an overview of a website's structure.
- **Depth-First Crawling**: Follows a single path of links as far as possible before backtracking. Useful for finding specific or deep content within a website.

## Extracting Valuable Information

Crawlers can gather various data, crucial for reconnaissance:

- **Links (Internal and External)**: Map out a website's structure and discover hidden pages.
- **Comments**: May contain sensitive information inadvertently revealed by users.
- **Metadata**: Provides context like page titles, descriptions, and keywords.
- **Sensitive Files**: Crawlers can search for exposed files like backups, configuration files, and logs.

## Importance of Context

The value of the extracted data lies in its context. For example, a comment mentioning software version combined with outdated metadata or a vulnerable configuration file can reveal a critical vulnerability. By analyzing data holistically, you can uncover hidden patterns and significant findings that might be overlooked if considered in isolation.

---------------------------------------------------------------------
# Well-Known URIs

The `.well-known` standard, defined in RFC 8615, is a directory within a website's root domain (accessible via `/.well-known/`). It centralizes critical metadata like configuration files, making it easier for browsers, applications, and security tools to locate and retrieve information. This directory is managed by IANA, which maintains a registry of specific `.well-known` URIs for various purposes.

### Notable .well-known URIs

- **security.txt**: Contact info for reporting vulnerabilities (RFC 9116).
- **change-password**: Standard URL for password change pages.
- **openid-configuration**: Configuration details for OpenID Connect (OAuth 2.0).
- **assetlinks.json**: Verifies ownership of digital assets linked to a domain.
- **mta-sts.txt**: Policy for SMTP MTA Strict Transport Security (RFC 8461).

### Web Recon and .well-known

In web reconnaissance, `.well-known` URIs are valuable for discovering endpoints and configuration details. For example, the `openid-configuration` URI, part of the OpenID Connect Discovery protocol, provides a JSON document with key metadata about authentication endpoints, token issuance, and supported features, offering numerous exploration opportunities during a penetration test.

Exploring these standardized URIs allows security professionals to map out a website's security landscape comprehensively.

---------------------------------------------------------------------

# Creepy Crawlies

Web crawling can be complex, but numerous tools are available to make the process faster and more efficient, allowing you to focus on analyzing the extracted data.

## Popular Web Crawlers

- **Burp Suite Spider**: Part of Burp Suite, it maps web applications, finds hidden content, and uncovers vulnerabilities.
- **OWASP ZAP**: A free, open-source security scanner with a spider component for crawling and identifying vulnerabilities.
- **Scrapy**: A Python framework for building custom web crawlers, ideal for complex and tailored tasks.
- **Apache Nutch**: A scalable Java-based crawler for large-scale web crawls, requiring more technical setup.

## Ethical Considerations

Always follow ethical practices when crawling websites. Obtain permission before performing scans and avoid overloading server resources.

```shell-session
$ pip3 install scrapy
```
```shell-session
$ wget -O ReconSpider.zip https://academy.hackthebox.com/storage/modules/144/ReconSpider.v1.2.zip
$ unzip ReconSpider.zip 
```
```shell-session
$ python3 ReconSpider.py http://inlanefreight.com
```


